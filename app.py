import gradio as gr
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import os
import zipfile
import shutil
import numpy as np

# **é»‘è‰²èƒŒæ™¯**
plt.style.use("dark_background")
sns.set(style="darkgrid")

# **æ•°æ®é›†å¤„ç†**
def process_dataset(dataset_zip):
    extract_path = "dataset"
    if os.path.exists(extract_path):
        shutil.rmtree(extract_path)
    os.makedirs(extract_path, exist_ok=True)
    with zipfile.ZipFile(dataset_zip.name, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    categories = [d for d in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, d))]
    if len(categories) < 2:
        return "Error: Dataset must contain at least 2 category folders.", None
    return f"Dataset extracted successfully! Found classes: {categories}", categories

# **è®­ç»ƒæ¨¡å‹**
def train_model(epochs, batch_size, learning_rate, dataset_zip):
    # æ•°æ®é›†å¤„ç†
    status, categories = process_dataset(dataset_zip)
    if categories is None:
        return status, None, None, None, None, None

    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])
    dataset_path = "dataset"
    train_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    num_classes = len(train_dataset.classes)
    
    # ç¡®ä¿è‡³å°‘æœ‰2ä¸ªç±»åˆ«
    if num_classes < 2:
        return "Training failed: At least two categories are required.", None, None, None, None, None

    # æ¨¡å‹å®šä¹‰ï¼ˆä½¿ç”¨ResNet50ï¼‰
    model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)
    model.fc = nn.Linear(model.fc.in_features, num_classes)  # ä¿®æ”¹æœ€åä¸€å±‚è¾“å‡ºç»´åº¦

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    loss_history = []
    acc_history = []
    all_preds = []
    all_labels = []
    training_log = ""

    # è®­ç»ƒè¿‡ç¨‹
    for epoch in range(int(epochs)):
        total_loss = 0
        correct = 0
        total = 0
        for images, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
            all_preds.extend(predicted.tolist())
            all_labels.extend(labels.tolist())
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total * 100
        loss_history.append(avg_loss)
        acc_history.append(accuracy)
        training_log += f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\n"

        # æ¯ä¸ªepochç»“æŸåæ›´æ–°è¿›åº¦
        yield gr.update(value=f"Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%"), None, None, None, None

    # ä¿å­˜æ¨¡å‹
    model_path = "trained_model.pth"
    torch.save(model.state_dict(), model_path)

    # **äº¤äº’å¼ Loss æ›²çº¿**
    loss_fig = go.Figure()
    loss_fig.add_trace(go.Scatter(x=list(range(1, int(epochs) + 1)), y=loss_history, mode='lines', name='Loss'))
    loss_fig.update_layout(title='Training Loss Curve', xaxis_title='Epoch', yaxis_title='Loss')

    # **äº¤äº’å¼ Accuracy æ›²çº¿**
    acc_fig = go.Figure()
    acc_fig.add_trace(go.Scatter(x=list(range(1, int(epochs) + 1)), y=acc_history, mode='lines', name='Accuracy'))
    acc_fig.update_layout(title='Training Accuracy Curve', xaxis_title='Epoch', yaxis_title='Accuracy (%)')

    # **æ··æ·†çŸ©é˜µ**
    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)
    for t, p in zip(all_labels, all_preds):
        confusion_matrix[t, p] += 1
    fig, ax = plt.subplots(figsize=(6, 6))
    sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="coolwarm", xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    confusion_matrix_path = "confusion_matrix.png"
    plt.savefig(confusion_matrix_path)

    return training_log, loss_fig, acc_fig, confusion_matrix_path, model_path


# **ç•Œé¢**
with gr.Blocks(theme=gr.themes.Monochrome()) as demo:
    gr.Markdown("# ğŸ”¥ Interactive Model Training Platform")

    dataset_upload = gr.File(label="Upload Dataset (ZIP Format)")
    dataset_output = gr.Textbox(label="Dataset Processing Status")
    category_list = gr.Textbox(label="Detected Categories")
    dataset_btn = gr.Button("Extract Dataset")
    dataset_btn.click(process_dataset, inputs=dataset_upload, outputs=[dataset_output, category_list])

    epochs = gr.Slider(minimum=1, maximum=100, step=1, value=10, label="Epochs")
    batch_size = gr.Slider(minimum=1, maximum=128, step=1, value=32, label="Batch Size")
    learning_rate = gr.Slider(minimum=0.0001, maximum=0.1, step=0.0001, value=0.001, label="Learning Rate")

    train_btn = gr.Button("Start Training")
    output_text = gr.Textbox(label="Training Status")
    loss_plot = gr.Plot(label="Loss Curve")
    acc_plot = gr.Plot(label="Accuracy Curve")
    confusion_matrix_img = gr.Image(label="Confusion Matrix")
    model_download = gr.File(label="Download Trained Model")

    # è®­ç»ƒæŒ‰é’®ç‚¹å‡»åå¼€å§‹è®­ç»ƒå¹¶æ˜¾ç¤ºè¿›åº¦
    train_btn.click(
        train_model,
        inputs=[epochs, batch_size, learning_rate, dataset_upload],
        outputs=[output_text, loss_plot, acc_plot, confusion_matrix_img, model_download]
    )

demo.launch()
